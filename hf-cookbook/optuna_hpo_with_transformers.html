<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hyperparameter Optimization with Optuna and Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="optuna_hpo_with_transformers_files/libs/clipboard/clipboard.min.js"></script>
<script src="optuna_hpo_with_transformers_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="optuna_hpo_with_transformers_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="optuna_hpo_with_transformers_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="optuna_hpo_with_transformers_files/libs/quarto-html/popper.min.js"></script>
<script src="optuna_hpo_with_transformers_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="optuna_hpo_with_transformers_files/libs/quarto-html/anchor.min.js"></script>
<link href="optuna_hpo_with_transformers_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="optuna_hpo_with_transformers_files/libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="optuna_hpo_with_transformers_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="optuna_hpo_with_transformers_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="optuna_hpo_with_transformers_files/libs/bootstrap/bootstrap-82fadaf3114e7bafe48a7052222a13f4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Hyperparameter Optimization with Optuna and Transformers</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em>Authored by: <a href="https://github.com/ParagEkbote">Parag Ekbote</a></em></p>
<section id="problem" class="level2">
<h2 class="anchored" data-anchor-id="problem"><strong>Problem:</strong></h2>
<p>Find the best hyperparameters to fine-tune a lightweight BERT model for text classification on a subset of the IMDB dataset.</p>
</section>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview"><strong>Overview:</strong></h2>
<p>This recipe demonstrates how to systematically optimize hyperparameters for transformer-based text classification models using automated search techniques. You’ll learn to implement HPO using Optuna to find optimal learning rates and weight decay values for fine-tuning BERT on sentiment analysis tasks.</p>
</section>
<section id="when-to-use-this-recipe" class="level2">
<h2 class="anchored" data-anchor-id="when-to-use-this-recipe"><strong>When to Use This Recipe:</strong></h2>
<ul>
<li><p>You need to fine-tune pre-trained language models for classification tasks.</p></li>
<li><p>Your model performance is plateauing and requires parameter refinement.</p></li>
<li><p>You want to implement systematic, reproducible hyperparameter optimization.</p></li>
</ul>
<section id="notes" class="level3">
<h3 class="anchored" data-anchor-id="notes">Notes</h3>
<ul>
<li>For detailed guidance on hyperparameter search with Transformers, refer to the <a href="https://huggingface.co/docs/transformers/en/hpo_train">Hugging Face HPO documentation</a>.</li>
</ul>
<div id="a309e1a0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q datasets evaluate transformers optuna wandb scikit<span class="op">-</span>learn nbformat matplotlib</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="prepare-dataset-and-set-model" class="level2">
<h2 class="anchored" data-anchor-id="prepare-dataset-and-set-model">Prepare Dataset and Set Model</h2>
<p>Before you can train and evaluate a sentiment analysis model, you’ll need to prep the dataset. This section ensures that your data is structured and your model is primed for learning from scratch or fine-tuning in the case of BERT.</p>
<section id="load-the-imdb-dataset" class="level3">
<h3 class="anchored" data-anchor-id="load-the-imdb-dataset">1. <strong>Load the IMDB Dataset</strong></h3>
<p>Begin by selecting a dataset focused on sentiment classification. IMDB is a well-known benchmark that features movie reviews labeled as either positive or negative.</p>
</section>
<section id="select-input-and-output-columns" class="level3">
<h3 class="anchored" data-anchor-id="select-input-and-output-columns">2. <strong>Select Input and Output Columns</strong></h3>
<p>Focus only on the essentials:<br>
- <code>text</code> column serves as the input (review content)<br>
- <code>label</code> column serves as the target (0 for negative, 1 for positive sentiment)</p>
</section>
<section id="define-the-trainvalidation-split" class="level3">
<h3 class="anchored" data-anchor-id="define-the-trainvalidation-split">3. <strong>Define the Train/Validation Split</strong></h3>
<p>Choose a consistent sampling strategy by selecting:<br>
- 2000 examples for training<br>
- 1000 examples for validation<br>
Use a fixed random seed when shuffling to ensure reproducibility across sessions.</p>
</section>
<section id="tokenize-the-dataset" class="level3">
<h3 class="anchored" data-anchor-id="tokenize-the-dataset">4. <strong>Tokenize the Dataset</strong></h3>
<p>Apply a tokenizer compatible with the model you’re planning to use. Tokenization converts raw text into numerical format so the model can ingest it effectively. Use batch processing to make this step efficient.</p>
</section>
<section id="load-an-evaluation-metric" class="level3">
<h3 class="anchored" data-anchor-id="load-an-evaluation-metric">5. <strong>Load an Evaluation Metric</strong></h3>
<p>Choose “accuracy” as the primary evaluation metric—simple and effective for binary classification tasks like this. It will later help gauge how well your model is learning the difference between positive and negative sentiment.</p>
</section>
<section id="initialize-a-pretrained-bert-model" class="level3">
<h3 class="anchored" data-anchor-id="initialize-a-pretrained-bert-model">6. <strong>Initialize a Pretrained BERT Model</strong></h3>
<p>Select a pretrained BERT-based model tailored for sequence classification tasks. Set the number of output classes to 2 (positive and negative) to align with your sentiment labels. This model will serve as the learner throughout the training process.</p>
<div id="2cfb9d5e" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> evaluate</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> set_seed</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">42</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> load_dataset(<span class="st">"imdb"</span>, split<span class="op">=</span><span class="st">"train"</span>).shuffle(seed<span class="op">=</span><span class="dv">42</span>).select(<span class="bu">range</span>(<span class="dv">2500</span>))</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>valid_dataset <span class="op">=</span> load_dataset(<span class="st">"imdb"</span>, split<span class="op">=</span><span class="st">"test"</span>).shuffle(seed<span class="op">=</span><span class="dv">42</span>).select(<span class="bu">range</span>(<span class="dv">1000</span>))</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"prajjwal1/bert-tiny"</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(batch):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(batch[<span class="st">"text"</span>], padding<span class="op">=</span><span class="st">"max_length"</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>tokenized_train <span class="op">=</span> train_dataset.<span class="bu">map</span>(tokenize, batched<span class="op">=</span><span class="va">True</span>).select_columns(</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"input_ids"</span>, <span class="st">"attention_mask"</span>, <span class="st">"label"</span>]</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>tokenized_valid <span class="op">=</span> valid_dataset.<span class="bu">map</span>(tokenize, batched<span class="op">=</span><span class="va">True</span>).select_columns(</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"input_ids"</span>, <span class="st">"attention_mask"</span>, <span class="st">"label"</span>]</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>metric <span class="op">=</span> evaluate.load(<span class="st">"accuracy"</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_init():</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> AutoModelForSequenceClassification.from_pretrained(model_name, num_labels<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8ee77ecb1a0f41c2a715840e76cd5727","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fd042613685f42dbbb6ff5c51d17b915","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"07fb47d86d11433aacc67d4782090b17","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d16d34949ee74f31bdc80b1bc75b2e19","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
</section>
<section id="define-storage-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="define-storage-with-optuna">Define Storage with Optuna</h2>
<p>To ensure your hyperparameter optimization experiments are trackable, reproducible, and easy to analyze over time, it’s essential to use a persistent storage backend. Optuna provides a robust solution for this through its <code>RDBStorage</code> mechanism, which allows saving trial data across multiple sessions using an SQLite database.</p>
<section id="choose-a-persistent-storage-format" class="level3">
<h3 class="anchored" data-anchor-id="choose-a-persistent-storage-format">1. <strong>Choose a Persistent Storage Format</strong></h3>
<p>Opt for an SQLite database as the storage medium. It’s lightweight, portable, and ideal for local experimentation, while still enabling structured access to all trial data.</p>
</section>
<section id="enable-optunas-rdbstorage" class="level3">
<h3 class="anchored" data-anchor-id="enable-optunas-rdbstorage">2. <strong>Enable Optuna’s RDBStorage</strong></h3>
<p>RDBStorage (Relational Database Storage) is Optuna’s way of saving trial results in a consistent and queryable format. This bridges the gap between short-term experimentation and long-term analysis.</p>
</section>
<section id="preserve-trial-history-across-sessions" class="level3">
<h3 class="anchored" data-anchor-id="preserve-trial-history-across-sessions">3. <strong>Preserve Trial History Across Sessions</strong></h3>
<p>By setting up persistent storage, you ensure that every hyperparameter trial is recorded. You can pause and resume studies, add more trials later, or analyze outcomes long after training has ended.</p>
</section>
<section id="facilitate-reproducible-analysis" class="level3">
<h3 class="anchored" data-anchor-id="facilitate-reproducible-analysis">4. <strong>Facilitate Reproducible Analysis</strong></h3>
<p>With trials stored centrally, you can revisit earlier results, regenerate visualizations, or compare different optimization runs. This makes your workflow transparent, collaborative, and scientifically rigorous.</p>
</section>
<section id="support-visualization-and-monitoring-tools" class="level3">
<h3 class="anchored" data-anchor-id="support-visualization-and-monitoring-tools">5. <strong>Support Visualization and Monitoring Tools</strong></h3>
<p>Storing trials persistently lets you plug in visualization tools—like Optuna’s built-in plotting utilities or external dashboards—to inspect performance trends and refine your search space iteratively.</p>
<div id="7652b74b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optuna.storages <span class="im">import</span> RDBStorage</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define persistent storage</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>storage <span class="op">=</span> RDBStorage(<span class="st">"sqlite:///optuna_trials.db"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    study_name<span class="op">=</span><span class="st">"transformers_optuna_study"</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    direction<span class="op">=</span><span class="st">"maximize"</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    storage<span class="op">=</span>storage,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    load_if_exists<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2025-06-25 15:10:18,724] A new study created in RDB with name: transformers_optuna_study</code></pre>
</div>
</div>
</section>
</section>
<section id="initialize-trainer-and-set-up-observability" class="level2">
<h2 class="anchored" data-anchor-id="initialize-trainer-and-set-up-observability">Initialize Trainer and Set Up Observability</h2>
<p>Now that your hyperparameter search space is in place, the next step is to wire everything together for optimization and tracking. This setup ensures not only accuracy-driven tuning but also full visibility into the training process using Weight &amp; Biases (W&amp;B).</p>
<section id="define-the-metric-function" class="level3">
<h3 class="anchored" data-anchor-id="define-the-metric-function">1. <strong>Define the Metric Function</strong></h3>
<p>Start by specifying how model performance will be measured. This metric function evaluates predictions after each validation step to calculate accuracy, F1-score, or loss. It becomes the feedback loop guiding each trial’s learning progress.</p>
</section>
<section id="construct-the-objective-function" class="level3">
<h3 class="anchored" data-anchor-id="construct-the-objective-function">2. <strong>Construct the Objective Function</strong></h3>
<p>This is the centerpiece of hyperparameter optimization. It wraps your training loop and returns a scalar score based on the chosen metric (like validation accuracy). Optuna will use this to compare trials and decide which settings yield the best outcomes.</p>
</section>
<section id="set-up-weight-biases-for-observability" class="level3">
<h3 class="anchored" data-anchor-id="set-up-weight-biases-for-observability">3. <strong>Set Up Weight &amp; Biases for Observability</strong></h3>
<p>Configure your environment to log experiment metrics and hyperparameter configurations to W&amp;B. This platform offers dashboards, plots, and experiment comparisons to track progress and spot issues.</p>
</section>
<section id="authenticate-for-logging-access" class="level3">
<h3 class="anchored" data-anchor-id="authenticate-for-logging-access">4. <strong>Authenticate for Logging Access</strong></h3>
<p>Log in to W&amp;B using your personal API key. This step connects your training session to your online account so that all metrics and trial details are properly tracked and stored.</p>
</section>
<section id="define-trainer-arguments" class="level3">
<h3 class="anchored" data-anchor-id="define-trainer-arguments">5. <strong>Define Trainer Arguments</strong></h3>
<p>Prepare a configuration setup for your training manager (such as the Hugging Face <code>Trainer</code>). Include settings for: - Evaluation strategy (e.g.&nbsp;after every epoch) - Checkpoint frequency and save conditions - Logging intervals - Hyperparameter search method and objectives</p>
<p>This ensures that training is robust, reproducible, and easy to resume or analyze.</p>
<div id="e0ee0bae" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> wandb</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer, TrainingArguments</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(eval_pred):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> eval_pred.predictions.argmax(axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> eval_pred.label_ids</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> metric.compute(predictions<span class="op">=</span>predictions, references<span class="op">=</span>labels)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_objective(metrics):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> metrics[<span class="st">"eval_accuracy"</span>]</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>wandb.init(project<span class="op">=</span><span class="st">"hf-optuna"</span>, name<span class="op">=</span><span class="st">"transformers_optuna_study"</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./results"</span>,</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        eval_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        logging_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        report_to<span class="op">=</span><span class="st">"wandb"</span>,  <span class="co"># Logs to W&amp;B</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        logging_dir<span class="op">=</span><span class="st">"./logs"</span>,</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        run_name<span class="op">=</span><span class="st">"transformers_optuna_study"</span>,</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    model_init<span class="op">=</span>model_init,</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized_train,</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>tokenized_valid,</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    processing_class<span class="op">=</span>tokenizer,</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<div class="ansi-escaped-output">
<pre><span class="ansi-blue-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">ai_novice2005</span> to <span class="ansi-green-fg">https://api.wandb.ai</span>. Use <span class="ansi-bold">`wandb login --relogin`</span> to force relogin
</pre>
</div>
</div>
<div class="cell-output cell-output-display">
Tracking run with wandb version 0.20.1
</div>
<div class="cell-output cell-output-display">
Run data is saved locally in <code>/teamspace/studios/this_studio/cookbook/notebooks/en/wandb/run-20250625_151029-ivr2ci8c</code>
</div>
<div class="cell-output cell-output-display">
Syncing run <strong><a href="https://wandb.ai/ai_novice2005/hf-optuna/runs/ivr2ci8c" target="_blank">transformers_optuna_study</a></strong> to <a href="https://wandb.ai/ai_novice2005/hf-optuna" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/developer-guide" target="_blank">docs</a>)<br>
</div>
<div class="cell-output cell-output-display">
 View project at <a href="https://wandb.ai/ai_novice2005/hf-optuna" target="_blank">https://wandb.ai/ai_novice2005/hf-optuna</a>
</div>
<div class="cell-output cell-output-display">
 View run at <a href="https://wandb.ai/ai_novice2005/hf-optuna/runs/ivr2ci8c" target="_blank">https://wandb.ai/ai_novice2005/hf-optuna/runs/ivr2ci8c</a>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2a4b6835e125481597ce8871f368aeeb","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a570d3ad87c44539a0eef1321dc34afc","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
</section>
<section id="define-search-space-and-start-trials" class="level2">
<h2 class="anchored" data-anchor-id="define-search-space-and-start-trials">Define Search Space and Start Trials</h2>
<p>Before diving into model training, it’s essential to thoughtfully define the ingredients and the exploration strategy. This step sets the stage for hyperparameter optimization using Optuna, where you’ll systematically explore combinations of training parameters like learning rate, weight decay, and batch size.</p>
<section id="design-the-search-space" class="level3">
<h3 class="anchored" data-anchor-id="design-the-search-space">1. <strong>Design the Search Space</strong></h3>
<p>Begin by outlining the hyperparameters you want to optimize. Choose reasonable lower and upper bounds for each: - <em>Learning rate</em>—controls the step size during optimization. - <em>Weight decay</em>—adds regularization to reduce overfitting. - <em>Batch size</em>—affects memory use and convergence stability.</p>
</section>
<section id="set-the-optimization-direction" class="level3">
<h3 class="anchored" data-anchor-id="set-the-optimization-direction">2. <strong>Set the Optimization Direction</strong></h3>
<p>Decide whether your goal is to minimize (e.g.&nbsp;loss) or maximize (e.g.&nbsp;accuracy, F1 score) the evaluation metric. This guides the search engine in the right direction.</p>
</section>
<section id="choose-optuna-as-the-backend" class="level3">
<h3 class="anchored" data-anchor-id="choose-optuna-as-the-backend">3. <strong>Choose Optuna as the Backend</strong></h3>
<p>Optuna will handle the search process—selecting, evaluating, and iterating through hyperparameter combinations intelligently.</p>
</section>
<section id="specify-the-number-of-trials" class="level3">
<h3 class="anchored" data-anchor-id="specify-the-number-of-trials">4. <strong>Specify the Number of Trials</strong></h3>
<p>Define how many individual runs (“trials”) you want Optuna to attempt. More trials can explore the space better but take more time.</p>
</section>
<section id="define-the-objective-function" class="level3">
<h3 class="anchored" data-anchor-id="define-the-objective-function">5. <strong>Define the Objective Function</strong></h3>
<p>This function calculates the metric to be optimized during each trial. It encapsulates how the model is trained and how performance is evaluated after each configuration is tested.</p>
</section>
<section id="name-the-study-for-persistence" class="level3">
<h3 class="anchored" data-anchor-id="name-the-study-for-persistence">6. <strong>Name the Study for Persistence</strong></h3>
<p>Assign a name to your study so it can be resumed or referenced later. This is especially useful when running experiments over multiple sessions or machines.</p>
</section>
<section id="set-up-persistent-storage" class="level3">
<h3 class="anchored" data-anchor-id="set-up-persistent-storage">7. <strong>Set Up Persistent Storage</strong></h3>
<p>Choose the storage backend that we previously setup to let you continue the study later, analyze results, or visualize metrics even after a system reboot.</p>
<div id="4ef88312" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> optuna_hp_space(trial):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"learning_rate"</span>: trial.suggest_float(<span class="st">"learning_rate"</span>, <span class="fl">1e-6</span>, <span class="fl">1e-4</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"per_device_train_batch_size"</span>: trial.suggest_categorical(</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">"per_device_train_batch_size"</span>, [<span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"weight_decay"</span>: trial.suggest_float(<span class="st">"weight_decay"</span>, <span class="fl">0.0</span>, <span class="fl">0.3</span>),</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>best_run <span class="op">=</span> trainer.hyperparameter_search(</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    direction<span class="op">=</span><span class="st">"maximize"</span>,</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    backend<span class="op">=</span><span class="st">"optuna"</span>,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    hp_space<span class="op">=</span>optuna_hp_space,</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    n_trials<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    compute_objective<span class="op">=</span>compute_objective,</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    study_name<span class="op">=</span><span class="st">"transformers_optuna_study"</span>,</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    storage<span class="op">=</span><span class="st">"sqlite:///optuna_trials.db"</span>,</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    load_if_exists<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(best_run)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2025-06-25 15:10:41,259] Using an existing study with name 'transformers_optuna_study' instead of creating a new one.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
 View run <strong style="color:#cdcd00">transformers_optuna_study</strong> at: <a href="https://wandb.ai/ai_novice2005/hf-optuna/runs/ivr2ci8c" target="_blank">https://wandb.ai/ai_novice2005/hf-optuna/runs/ivr2ci8c</a><br> View project at: <a href="https://wandb.ai/ai_novice2005/hf-optuna" target="_blank">https://wandb.ai/ai_novice2005/hf-optuna</a><br>Synced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
</div>
<div class="cell-output cell-output-display">
Find logs at: <code>./wandb/run-20250625_151029-ivr2ci8c/logs</code>
</div>
<div class="cell-output cell-output-display">
Tracking run with wandb version 0.20.1
</div>
<div class="cell-output cell-output-display">
Run data is saved locally in <code>/teamspace/studios/this_studio/cookbook/notebooks/en/wandb/run-20250625_151042-up8j8xgb</code>
</div>
<div class="cell-output cell-output-display">
Syncing run <strong><a href="https://wandb.ai/ai_novice2005/huggingface/runs/up8j8xgb" target="_blank">leafy-breeze-5</a></strong> to <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/developer-guide" target="_blank">docs</a>)<br>
</div>
<div class="cell-output cell-output-display">
 View project at <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">https://wandb.ai/ai_novice2005/huggingface</a>
</div>
<div class="cell-output cell-output-display">
 View run at <a href="https://wandb.ai/ai_novice2005/huggingface/runs/up8j8xgb" target="_blank">https://wandb.ai/ai_novice2005/huggingface/runs/up8j8xgb</a>
</div>
<div class="cell-output cell-output-display">
<div>
      
      <progress value="471" max="471" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [471/471 00:24, Epoch 3/3]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.649200</td>
<td>0.605374</td>
<td>0.682000</td>
</tr>
<tr class="even">
<td>2</td>
<td>0.529900</td>
<td>0.528273</td>
<td>0.751000</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0.440700</td>
<td>0.509003</td>
<td>0.764000</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2025-06-25 15:11:09,298] Trial 0 finished with value: 0.764 and parameters: {'learning_rate': 7.23655165533393e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.013798094328723032}. Best is trial 0 with value: 0.764.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class="wandb-row"><div class="wandb-col"><h3 class="anchored">Run history:</h3><br>
<table class="wandb caption-top table table-sm table-striped small">
<tbody>
<tr class="odd">
<td>eval/accuracy</td>
<td>▁▇█</td>
</tr>
<tr class="even">
<td>eval/loss</td>
<td>█▂▁</td>
</tr>
<tr class="odd">
<td>eval/runtime</td>
<td>█▇▁</td>
</tr>
<tr class="even">
<td>eval/samples_per_second</td>
<td>▁▂█</td>
</tr>
<tr class="odd">
<td>eval/steps_per_second</td>
<td>▁▂█</td>
</tr>
<tr class="even">
<td>train/epoch</td>
<td>▁▁▅▅███</td>
</tr>
<tr class="odd">
<td>train/global_step</td>
<td>▁▁▄▄███</td>
</tr>
<tr class="even">
<td>train/grad_norm</td>
<td>▁█▂</td>
</tr>
<tr class="odd">
<td>train/learning_rate</td>
<td>█▅▁</td>
</tr>
<tr class="even">
<td>train/loss</td>
<td>█▄▁</td>
</tr>
</tbody>
</table>
<br></div><div class="wandb-col"><h3 class="anchored">Run summary:</h3><br>
<table class="wandb caption-top table table-sm table-striped small">
<tbody>
<tr class="odd">
<td>eval/accuracy</td>
<td>0.764</td>
</tr>
<tr class="even">
<td>eval/loss</td>
<td>0.509</td>
</tr>
<tr class="odd">
<td>eval/runtime</td>
<td>1.0937</td>
</tr>
<tr class="even">
<td>eval/samples_per_second</td>
<td>914.299</td>
</tr>
<tr class="odd">
<td>eval/steps_per_second</td>
<td>114.287</td>
</tr>
<tr class="even">
<td>total_flos</td>
<td>9528652800000.0</td>
</tr>
<tr class="odd">
<td>train/epoch</td>
<td>3</td>
</tr>
<tr class="even">
<td>train/global_step</td>
<td>471</td>
</tr>
<tr class="odd">
<td>train/grad_norm</td>
<td>13.57101</td>
</tr>
<tr class="even">
<td>train/learning_rate</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>train/loss</td>
<td>0.4407</td>
</tr>
<tr class="even">
<td>train_loss</td>
<td>0.53993</td>
</tr>
<tr class="odd">
<td>train_runtime</td>
<td>26.9493</td>
</tr>
<tr class="even">
<td>train_samples_per_second</td>
<td>278.3</td>
</tr>
<tr class="odd">
<td>train_steps_per_second</td>
<td>17.477</td>
</tr>
</tbody>
</table>
<br></div></div>
</div>
<div class="cell-output cell-output-display">
 View run <strong style="color:#cdcd00">leafy-breeze-5</strong> at: <a href="https://wandb.ai/ai_novice2005/huggingface/runs/up8j8xgb" target="_blank">https://wandb.ai/ai_novice2005/huggingface/runs/up8j8xgb</a><br> View project at: <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">https://wandb.ai/ai_novice2005/huggingface</a><br>Synced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
</div>
<div class="cell-output cell-output-display">
Find logs at: <code>./wandb/run-20250625_151042-up8j8xgb/logs</code>
</div>
<div class="cell-output cell-output-display">
Tracking run with wandb version 0.20.1
</div>
<div class="cell-output cell-output-display">
Run data is saved locally in <code>/teamspace/studios/this_studio/cookbook/notebooks/en/wandb/run-20250625_151110-1dgqb1s1</code>
</div>
<div class="cell-output cell-output-display">
Syncing run <strong><a href="https://wandb.ai/ai_novice2005/huggingface/runs/1dgqb1s1" target="_blank">radiant-sound-6</a></strong> to <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/developer-guide" target="_blank">docs</a>)<br>
</div>
<div class="cell-output cell-output-display">
 View project at <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">https://wandb.ai/ai_novice2005/huggingface</a>
</div>
<div class="cell-output cell-output-display">
 View run at <a href="https://wandb.ai/ai_novice2005/huggingface/runs/1dgqb1s1" target="_blank">https://wandb.ai/ai_novice2005/huggingface/runs/1dgqb1s1</a>
</div>
<div class="cell-output cell-output-display">
<div>
      
      <progress value="60" max="60" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [60/60 00:18, Epoch 3/3]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.683100</td>
<td>0.677468</td>
<td>0.613000</td>
</tr>
<tr class="even">
<td>2</td>
<td>0.673100</td>
<td>0.669755</td>
<td>0.639000</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0.669500</td>
<td>0.667655</td>
<td>0.630000</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2025-06-25 15:11:29,907] Trial 1 finished with value: 0.63 and parameters: {'learning_rate': 2.756288216246014e-05, 'per_device_train_batch_size': 128, 'weight_decay': 0.28503663896216014}. Best is trial 0 with value: 0.764.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class="wandb-row"><div class="wandb-col"><h3 class="anchored">Run history:</h3><br>
<table class="wandb caption-top table table-sm table-striped small">
<tbody>
<tr class="odd">
<td>eval/accuracy</td>
<td>▁█▆</td>
</tr>
<tr class="even">
<td>eval/loss</td>
<td>█▂▁</td>
</tr>
<tr class="odd">
<td>eval/runtime</td>
<td>▁█▂</td>
</tr>
<tr class="even">
<td>eval/samples_per_second</td>
<td>█▁▇</td>
</tr>
<tr class="odd">
<td>eval/steps_per_second</td>
<td>█▁▇</td>
</tr>
<tr class="even">
<td>train/epoch</td>
<td>▁▁▅▅███</td>
</tr>
<tr class="odd">
<td>train/global_step</td>
<td>▁▁▅▅███</td>
</tr>
<tr class="even">
<td>train/grad_norm</td>
<td>▅█▁</td>
</tr>
<tr class="odd">
<td>train/learning_rate</td>
<td>█▄▁</td>
</tr>
<tr class="even">
<td>train/loss</td>
<td>█▃▁</td>
</tr>
</tbody>
</table>
<br></div><div class="wandb-col"><h3 class="anchored">Run summary:</h3><br>
<table class="wandb caption-top table table-sm table-striped small">
<tbody>
<tr class="odd">
<td>eval/accuracy</td>
<td>0.63</td>
</tr>
<tr class="even">
<td>eval/loss</td>
<td>0.66765</td>
</tr>
<tr class="odd">
<td>eval/runtime</td>
<td>1.111</td>
</tr>
<tr class="even">
<td>eval/samples_per_second</td>
<td>900.116</td>
</tr>
<tr class="odd">
<td>eval/steps_per_second</td>
<td>112.515</td>
</tr>
<tr class="even">
<td>total_flos</td>
<td>9528652800000.0</td>
</tr>
<tr class="odd">
<td>train/epoch</td>
<td>3</td>
</tr>
<tr class="even">
<td>train/global_step</td>
<td>60</td>
</tr>
<tr class="odd">
<td>train/grad_norm</td>
<td>0.66353</td>
</tr>
<tr class="even">
<td>train/learning_rate</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>train/loss</td>
<td>0.6695</td>
</tr>
<tr class="even">
<td>train_loss</td>
<td>0.67521</td>
</tr>
<tr class="odd">
<td>train_runtime</td>
<td>19.5595</td>
</tr>
<tr class="even">
<td>train_samples_per_second</td>
<td>383.445</td>
</tr>
<tr class="odd">
<td>train_steps_per_second</td>
<td>3.068</td>
</tr>
</tbody>
</table>
<br></div></div>
</div>
<div class="cell-output cell-output-display">
 View run <strong style="color:#cdcd00">radiant-sound-6</strong> at: <a href="https://wandb.ai/ai_novice2005/huggingface/runs/1dgqb1s1" target="_blank">https://wandb.ai/ai_novice2005/huggingface/runs/1dgqb1s1</a><br> View project at: <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">https://wandb.ai/ai_novice2005/huggingface</a><br>Synced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
</div>
<div class="cell-output cell-output-display">
Find logs at: <code>./wandb/run-20250625_151110-1dgqb1s1/logs</code>
</div>
<div class="cell-output cell-output-display">
Tracking run with wandb version 0.20.1
</div>
<div class="cell-output cell-output-display">
Run data is saved locally in <code>/teamspace/studios/this_studio/cookbook/notebooks/en/wandb/run-20250625_151130-jt5mavd3</code>
</div>
<div class="cell-output cell-output-display">
Syncing run <strong><a href="https://wandb.ai/ai_novice2005/huggingface/runs/jt5mavd3" target="_blank">ancient-dream-7</a></strong> to <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/developer-guide" target="_blank">docs</a>)<br>
</div>
<div class="cell-output cell-output-display">
 View project at <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">https://wandb.ai/ai_novice2005/huggingface</a>
</div>
<div class="cell-output cell-output-display">
 View run at <a href="https://wandb.ai/ai_novice2005/huggingface/runs/jt5mavd3" target="_blank">https://wandb.ai/ai_novice2005/huggingface/runs/jt5mavd3</a>
</div>
<div class="cell-output cell-output-display">
<div>
      
      <progress value="120" max="120" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [120/120 00:20, Epoch 3/3]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.690300</td>
<td>0.688425</td>
<td>0.553000</td>
</tr>
<tr class="even">
<td>2</td>
<td>0.689100</td>
<td>0.687775</td>
<td>0.562000</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0.689100</td>
<td>0.687576</td>
<td>0.570000</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2025-06-25 15:11:52,797] Trial 2 finished with value: 0.57 and parameters: {'learning_rate': 1.2177346043359053e-06, 'per_device_train_batch_size': 64, 'weight_decay': 0.02906341093983704}. Best is trial 0 with value: 0.764.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class="wandb-row"><div class="wandb-col"><h3 class="anchored">Run history:</h3><br>
<table class="wandb caption-top table table-sm table-striped small">
<tbody>
<tr class="odd">
<td>eval/accuracy</td>
<td>▁▅█</td>
</tr>
<tr class="even">
<td>eval/loss</td>
<td>█▃▁</td>
</tr>
<tr class="odd">
<td>eval/runtime</td>
<td>▁██</td>
</tr>
<tr class="even">
<td>eval/samples_per_second</td>
<td>█▁▁</td>
</tr>
<tr class="odd">
<td>eval/steps_per_second</td>
<td>█▁▁</td>
</tr>
<tr class="even">
<td>train/epoch</td>
<td>▁▁▅▅███</td>
</tr>
<tr class="odd">
<td>train/global_step</td>
<td>▁▁▅▅███</td>
</tr>
<tr class="even">
<td>train/grad_norm</td>
<td>▆█▁</td>
</tr>
<tr class="odd">
<td>train/learning_rate</td>
<td>█▅▁</td>
</tr>
<tr class="even">
<td>train/loss</td>
<td>█▁▁</td>
</tr>
</tbody>
</table>
<br></div><div class="wandb-col"><h3 class="anchored">Run summary:</h3><br>
<table class="wandb caption-top table table-sm table-striped small">
<tbody>
<tr class="odd">
<td>eval/accuracy</td>
<td>0.57</td>
</tr>
<tr class="even">
<td>eval/loss</td>
<td>0.68758</td>
</tr>
<tr class="odd">
<td>eval/runtime</td>
<td>1.0959</td>
</tr>
<tr class="even">
<td>eval/samples_per_second</td>
<td>912.502</td>
</tr>
<tr class="odd">
<td>eval/steps_per_second</td>
<td>114.063</td>
</tr>
<tr class="even">
<td>total_flos</td>
<td>9528652800000.0</td>
</tr>
<tr class="odd">
<td>train/epoch</td>
<td>3</td>
</tr>
<tr class="even">
<td>train/global_step</td>
<td>120</td>
</tr>
<tr class="odd">
<td>train/grad_norm</td>
<td>2.34479</td>
</tr>
<tr class="even">
<td>train/learning_rate</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>train/loss</td>
<td>0.6891</td>
</tr>
<tr class="even">
<td>train_loss</td>
<td>0.68947</td>
</tr>
<tr class="odd">
<td>train_runtime</td>
<td>21.6288</td>
</tr>
<tr class="even">
<td>train_samples_per_second</td>
<td>346.76</td>
</tr>
<tr class="odd">
<td>train_steps_per_second</td>
<td>5.548</td>
</tr>
</tbody>
</table>
<br></div></div>
</div>
<div class="cell-output cell-output-display">
 View run <strong style="color:#cdcd00">ancient-dream-7</strong> at: <a href="https://wandb.ai/ai_novice2005/huggingface/runs/jt5mavd3" target="_blank">https://wandb.ai/ai_novice2005/huggingface/runs/jt5mavd3</a><br> View project at: <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">https://wandb.ai/ai_novice2005/huggingface</a><br>Synced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
</div>
<div class="cell-output cell-output-display">
Find logs at: <code>./wandb/run-20250625_151130-jt5mavd3/logs</code>
</div>
<div class="cell-output cell-output-display">
Tracking run with wandb version 0.20.1
</div>
<div class="cell-output cell-output-display">
Run data is saved locally in <code>/teamspace/studios/this_studio/cookbook/notebooks/en/wandb/run-20250625_151153-6eexo8uv</code>
</div>
<div class="cell-output cell-output-display">
Syncing run <strong><a href="https://wandb.ai/ai_novice2005/huggingface/runs/6eexo8uv" target="_blank">grateful-sound-8</a></strong> to <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/developer-guide" target="_blank">docs</a>)<br>
</div>
<div class="cell-output cell-output-display">
 View project at <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">https://wandb.ai/ai_novice2005/huggingface</a>
</div>
<div class="cell-output cell-output-display">
 View run at <a href="https://wandb.ai/ai_novice2005/huggingface/runs/6eexo8uv" target="_blank">https://wandb.ai/ai_novice2005/huggingface/runs/6eexo8uv</a>
</div>
<div class="cell-output cell-output-display">
<div>
      
      <progress value="120" max="120" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [120/120 00:16, Epoch 3/3]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.689400</td>
<td>0.686730</td>
<td>0.570000</td>
</tr>
<tr class="even">
<td>2</td>
<td>0.687000</td>
<td>0.685327</td>
<td>0.581000</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0.686700</td>
<td>0.684904</td>
<td>0.581000</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2025-06-25 15:12:12,894] Trial 3 finished with value: 0.581 and parameters: {'learning_rate': 2.973185825213819e-06, 'per_device_train_batch_size': 64, 'weight_decay': 0.09102292466460353}. Best is trial 0 with value: 0.764.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class="wandb-row"><div class="wandb-col"><h3 class="anchored">Run history:</h3><br>
<table class="wandb caption-top table table-sm table-striped small">
<tbody>
<tr class="odd">
<td>eval/accuracy</td>
<td>▁██</td>
</tr>
<tr class="even">
<td>eval/loss</td>
<td>█▃▁</td>
</tr>
<tr class="odd">
<td>eval/runtime</td>
<td>▁█▃</td>
</tr>
<tr class="even">
<td>eval/samples_per_second</td>
<td>█▁▆</td>
</tr>
<tr class="odd">
<td>eval/steps_per_second</td>
<td>█▁▆</td>
</tr>
<tr class="even">
<td>train/epoch</td>
<td>▁▁▅▅███</td>
</tr>
<tr class="odd">
<td>train/global_step</td>
<td>▁▁▅▅███</td>
</tr>
<tr class="even">
<td>train/grad_norm</td>
<td>▆█▁</td>
</tr>
<tr class="odd">
<td>train/learning_rate</td>
<td>█▅▁</td>
</tr>
<tr class="even">
<td>train/loss</td>
<td>█▂▁</td>
</tr>
</tbody>
</table>
<br></div><div class="wandb-col"><h3 class="anchored">Run summary:</h3><br>
<table class="wandb caption-top table table-sm table-striped small">
<tbody>
<tr class="odd">
<td>eval/accuracy</td>
<td>0.581</td>
</tr>
<tr class="even">
<td>eval/loss</td>
<td>0.6849</td>
</tr>
<tr class="odd">
<td>eval/runtime</td>
<td>1.0808</td>
</tr>
<tr class="even">
<td>eval/samples_per_second</td>
<td>925.219</td>
</tr>
<tr class="odd">
<td>eval/steps_per_second</td>
<td>115.652</td>
</tr>
<tr class="even">
<td>total_flos</td>
<td>9528652800000.0</td>
</tr>
<tr class="odd">
<td>train/epoch</td>
<td>3</td>
</tr>
<tr class="even">
<td>train/global_step</td>
<td>120</td>
</tr>
<tr class="odd">
<td>train/grad_norm</td>
<td>2.30065</td>
</tr>
<tr class="even">
<td>train/learning_rate</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>train/loss</td>
<td>0.6867</td>
</tr>
<tr class="even">
<td>train_loss</td>
<td>0.68768</td>
</tr>
<tr class="odd">
<td>train_runtime</td>
<td>18.9103</td>
</tr>
<tr class="even">
<td>train_samples_per_second</td>
<td>396.61</td>
</tr>
<tr class="odd">
<td>train_steps_per_second</td>
<td>6.346</td>
</tr>
</tbody>
</table>
<br></div></div>
</div>
<div class="cell-output cell-output-display">
 View run <strong style="color:#cdcd00">grateful-sound-8</strong> at: <a href="https://wandb.ai/ai_novice2005/huggingface/runs/6eexo8uv" target="_blank">https://wandb.ai/ai_novice2005/huggingface/runs/6eexo8uv</a><br> View project at: <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">https://wandb.ai/ai_novice2005/huggingface</a><br>Synced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
</div>
<div class="cell-output cell-output-display">
Find logs at: <code>./wandb/run-20250625_151153-6eexo8uv/logs</code>
</div>
<div class="cell-output cell-output-display">
Tracking run with wandb version 0.20.1
</div>
<div class="cell-output cell-output-display">
Run data is saved locally in <code>/teamspace/studios/this_studio/cookbook/notebooks/en/wandb/run-20250625_151213-5w18j6iv</code>
</div>
<div class="cell-output cell-output-display">
Syncing run <strong><a href="https://wandb.ai/ai_novice2005/huggingface/runs/5w18j6iv" target="_blank">hopeful-moon-9</a></strong> to <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/developer-guide" target="_blank">docs</a>)<br>
</div>
<div class="cell-output cell-output-display">
 View project at <a href="https://wandb.ai/ai_novice2005/huggingface" target="_blank">https://wandb.ai/ai_novice2005/huggingface</a>
</div>
<div class="cell-output cell-output-display">
 View run at <a href="https://wandb.ai/ai_novice2005/huggingface/runs/5w18j6iv" target="_blank">https://wandb.ai/ai_novice2005/huggingface/runs/5w18j6iv</a>
</div>
<div class="cell-output cell-output-display">
<div>
      
      <progress value="120" max="120" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [120/120 00:17, Epoch 3/3]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.689000</td>
<td>0.686028</td>
<td>0.573000</td>
</tr>
<tr class="even">
<td>2</td>
<td>0.686100</td>
<td>0.684337</td>
<td>0.589000</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0.685700</td>
<td>0.683833</td>
<td>0.597000</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2025-06-25 15:12:32,824] Trial 4 finished with value: 0.597 and parameters: {'learning_rate': 3.763988365260261e-06, 'per_device_train_batch_size': 64, 'weight_decay': 0.1502192542358606}. Best is trial 0 with value: 0.764.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>BestRun(run_id='0', objective=0.764, hyperparameters={'learning_rate': 7.23655165533393e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.013798094328723032}, run_summary=None)</code></pre>
</div>
</div>
</section>
</section>
<section id="visualize-results" class="level2">
<h2 class="anchored" data-anchor-id="visualize-results">Visualize Results</h2>
<p>Once your Optuna study completes its trials, it’s time to peel back the layers and interpret what happened. Visualization brings clarity to how hyperparameters shaped the outcome and uncovers patterns that might otherwise stay buried in raw data.</p>
<section id="track-optimization-progress" class="level3">
<h3 class="anchored" data-anchor-id="track-optimization-progress">1. <strong>Track Optimization Progress</strong></h3>
<p>Use the optimization history to see how objective scores evolved over trials. This helps you understand whether performance steadily improved, plateaued, or oscillated. It’s your window into the pace and trajectory of the search process.</p>
</section>
<section id="inspect-training-behavior-via-intermediate-values" class="level3">
<h3 class="anchored" data-anchor-id="inspect-training-behavior-via-intermediate-values">2. <strong>Inspect Training Behavior via Intermediate Values</strong></h3>
<p>If your model reports evaluation metrics during training (like per epoch), intermediate value plots let you monitor how each trial performed in real time. This is especially valuable for early-stopping decisions and assessing learning stability.</p>
</section>
<section id="reveal-key-hyperparameters-through-importance-rankings" class="level3">
<h3 class="anchored" data-anchor-id="reveal-key-hyperparameters-through-importance-rankings">3. <strong>Reveal Key Hyperparameters through Importance Rankings</strong></h3>
<p>Parameter importance plots uncover which hyperparameters actually mattered—did tweaking the learning rate move the needle, or was batch size the star? Understanding this lets you simplify or refine your future search space.</p>
<div id="a8f14007" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optuna.visualization.matplotlib <span class="im">import</span> (</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    plot_optimization_history,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    plot_intermediate_values,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    plot_param_importances</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the study from RDB storage</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>storage <span class="op">=</span> optuna.storages.RDBStorage(<span class="st">"sqlite:///optuna_trials.db"</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.load_study(</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    study_name<span class="op">=</span><span class="st">"transformers_optuna_study"</span>,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    storage<span class="op">=</span>storage</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot optimization history</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> plot_optimization_history(study)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>ax1.figure.savefig(<span class="st">"optimization_history.png"</span>)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot intermediate values (if using pruning and intermediate reports)</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> plot_intermediate_values(study)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>ax2.figure.savefig(<span class="st">"intermediate_values.png"</span>)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot parameter importances</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>ax3 <span class="op">=</span> plot_param_importances(study)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>ax3.figure.savefig(<span class="st">"param_importances.png"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_16014/3851300317.py:18: ExperimentalWarning: plot_optimization_history is experimental (supported from v2.2.0). The interface can change in the future.
  ax1 = plot_optimization_history(study)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="optuna_hpo_with_transformers_files/figure-html/cell-7-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_16014/3851300317.py:23: ExperimentalWarning: plot_intermediate_values is experimental (supported from v2.2.0). The interface can change in the future.
  ax2 = plot_intermediate_values(study)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="optuna_hpo_with_transformers_files/figure-html/cell-7-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_16014/3851300317.py:28: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.
  ax3 = plot_param_importances(study)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="optuna_hpo_with_transformers_files/figure-html/cell-7-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="perform-final-training" class="level2">
<h2 class="anchored" data-anchor-id="perform-final-training">Perform Final Training</h2>
<p>Once you’ve completed hyperparameter optimization with Optuna, it’s time to capitalize on your best findings and carry out the final round of training.</p>
<section id="retrieve-your-ingredients" class="level3">
<h3 class="anchored" data-anchor-id="retrieve-your-ingredients">1. <strong>Retrieve Your Ingredients</strong></h3>
<p>Access the best set of hyperparameters identified during the tuning process.</p>
</section>
<section id="configure-training-parameters" class="level3">
<h3 class="anchored" data-anchor-id="configure-training-parameters">2. <strong>Configure Training Parameters</strong></h3>
<p>Plug those hyperparameter values into your training setup. This might include adjustments to learning rate, batch size, number of epochs, dropout rate, and other model-specific knobs that influence training behavior.</p>
</section>
<section id="incorporate-into-model-setup" class="level3">
<h3 class="anchored" data-anchor-id="incorporate-into-model-setup">3. <strong>Incorporate into Model Setup</strong></h3>
<p>Apply the optimized values to initialize and configure your model. This ensures your final training run is guided by the most effective settings discovered through trial and error.</p>
</section>
<section id="fine-tune-your-training-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="fine-tune-your-training-pipeline">4. <strong>Fine-Tune Your Training Pipeline</strong></h3>
<p>Set up your optimizer, loss function, and data loaders using the best parameters. Everything from how fast your model learns to how much data it sees at once should reflect your refined configuration.</p>
</section>
<section id="run-full-training" class="level3">
<h3 class="anchored" data-anchor-id="run-full-training">5. <strong>Run Full Training</strong></h3>
<p>Begin training your model using the entire training dataset (or at least the train/validation split you used during HPO). This pass should reflect your best shot at learning the patterns in the data without exploratory variation.</p>
<div id="6cd2f800" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load IMDb dataset</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"imdb"</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load tokenizer</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize the text</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(example):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(example[<span class="st">"text"</span>], padding<span class="op">=</span><span class="st">"max_length"</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply tokenization</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean up columns</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> tokenized_dataset.remove_columns([<span class="st">"text"</span>])</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> tokenized_dataset.rename_column(<span class="st">"label"</span>, <span class="st">"labels"</span>)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Set PyTorch format</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>tokenized_dataset.set_format(<span class="st">"torch"</span>, columns<span class="op">=</span>[<span class="st">"input_ids"</span>, <span class="st">"attention_mask"</span>, <span class="st">"labels"</span>])</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset for quick testing (optional)</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> tokenized_dataset[<span class="st">"train"</span>].shuffle(seed<span class="op">=</span><span class="dv">42</span>).select(<span class="bu">range</span>(<span class="dv">2000</span>))</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>valid_dataset <span class="op">=</span> tokenized_dataset[<span class="st">"test"</span>].shuffle(seed<span class="op">=</span><span class="dv">42</span>).select(<span class="bu">range</span>(<span class="dv">500</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8493882fcaf841f7bf64606edcf2b28e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3626375d66dd49b79684b966e0f190dd","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f98a9164e26c432da1cba82ab0dc874e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div id="07849a94" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(<span class="st">"bert-base-uncased"</span>, num_labels<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load best hyperparameters (already defined earlier as best_hparams)</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./final_model"</span>,</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span>best_hparams[<span class="st">"learning_rate"</span>],</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span>best_hparams[<span class="st">"per_device_train_batch_size"</span>],</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span>best_hparams[<span class="st">"weight_decay"</span>],    </span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    logging_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"wandb"</span>,</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    run_name<span class="op">=</span><span class="st">"final_run_with_best_hparams"</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Trainer</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>valid_dataset,</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    processing_class<span class="op">=</span>tokenizer,  </span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    compute_metrics<span class="op">=</span><span class="kw">lambda</span> eval_pred: {</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        <span class="st">"accuracy"</span>: (eval_pred.predictions.argmax(<span class="op">-</span><span class="dv">1</span>) <span class="op">==</span> eval_pred.label_ids).mean()</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Train</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>trainer.save_model(<span class="st">"./final_model"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipykernel_5615/1607841219.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
      
      <progress value="375" max="375" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [375/375 09:21, Epoch 3/3]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.456700</td>
<td>0.337511</td>
<td>0.856000</td>
</tr>
<tr class="even">
<td>2</td>
<td>0.215200</td>
<td>0.438220</td>
<td>0.876000</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0.084600</td>
<td>0.499159</td>
<td>0.888000</td>
</tr>
</tbody>
</table>
<p>
</p></div>
</div>
</section>
</section>
<section id="uploading-to-hugging-face-hub" class="level2">
<h2 class="anchored" data-anchor-id="uploading-to-hugging-face-hub">Uploading to Hugging Face Hub</h2>
<p>You’ve successfully trained a powerful and optimized model, it’s time to serve it up to the world. Sharing your model on the Hugging Face Hub not only makes it reusable and accessible for inference, but also contributes to the open-source community.</p>
<section id="celebrate-the-optimization-payoff" class="level3">
<h3 class="anchored" data-anchor-id="celebrate-the-optimization-payoff">1. <strong>Celebrate the Optimization Payoff</strong></h3>
<p>After rigorous tuning and final training, your model now performs more efficiently and consistently. These improvements make it ideal for real-world tasks such as sentiment analysis, like classifying movie reviews to fine-tune content recommendations.</p>
</section>
<section id="save-your-work-locally" class="level3">
<h3 class="anchored" data-anchor-id="save-your-work-locally">2. <strong>Save Your Work Locally</strong></h3>
<p>Before sharing, save the trained model—including the weights, configuration, tokenizer (if applicable), and training artifacts—on your local system. This step ensures that your model setup is reproducible and ready to be uploaded.</p>
</section>
<section id="authenticate-with-hugging-face-hub" class="level3">
<h3 class="anchored" data-anchor-id="authenticate-with-hugging-face-hub">3. <strong>Authenticate with Hugging Face Hub</strong></h3>
<p>To upload your model, you’ll need to log in to the Hugging Face Hub. Whether through a terminal or notebook interface, authentication links your environment to your personal or organizational space on the platform, enabling push access.</p>
</section>
<section id="upload-and-share" class="level3">
<h3 class="anchored" data-anchor-id="upload-and-share">4. <strong>Upload and Share</strong></h3>
<p>Push your saved model to the Hugging Face Hub. This makes the model publicly accessible—or private and enables others to load, use, and fine-tune it. You’ll also create a model card to explain what the model does, its intended use cases, and performance benchmarks.</p>
<section id="why-it-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters">📌 Why It Matters:</h4>
<ul>
<li>Centralized model storage encourages versioning, reproducibility, and transparency.</li>
<li>The Hub simplifies integration for downstream tasks through <code>transformers</code>compatible APIs.</li>
<li>Sharing models builds your profile and supports collaboration within the machine learning community.</li>
</ul>
<div id="e524424b" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForSequenceClassification</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load your saved model from the path</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(<span class="st">"./final_model"</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"./final_model"</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Push to your repository on the hub</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>model.push_to_hub(<span class="st">"AINovice2005/bert-imdb-optuna-hpo"</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>tokenizer.push_to_hub(<span class="st">"AINovice2005/bert-imdb-optuna-hpo"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"cb7eabc38cd845929c74f7f52ae98aba","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"35d0f752cae94cea9082ba475c1940d8","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>CommitInfo(commit_url='https://huggingface.co/AINovice2005/bert-imdb-optuna-hpo/commit/cf4e9bcfd581cc9cd33f7403c5fa2e5074f58e6c', commit_message='Upload tokenizer', commit_description='', oid='cf4e9bcfd581cc9cd33f7403c5fa2e5074f58e6c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AINovice2005/bert-imdb-optuna-hpo', endpoint='https://huggingface.co', repo_type='model', repo_id='AINovice2005/bert-imdb-optuna-hpo'), pr_revision=None, pr_num=None)</code></pre>
</div>
</div>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>